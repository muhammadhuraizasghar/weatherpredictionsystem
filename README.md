The Weather System ingests live satellite radiance every five minutes through an encrypted HTTPS channel secured by a rotating JWT token issued by the provider. A lightweight Python service running inside a 40 MB Alpine container opens an asyncio session with aiohttp and concurrently fetches 180 tiles covering the continental United States. Each tile is 256 by 256 pixels stored as 16-bit grayscale and weighs roughly 64 kB. The service streams the raw bytes into a Kafka topic called raw-satellite with snappy compression enabled. No intermediate files touch the disk which keeps the container ephemeral and the latency below two seconds end-to-end. The same binary runs on a Raspberry Pi on a fishing boat and on a 96-core cloud instance because the only dependency is a single requirements.txt with aiohttp kafka-python pillow numpy and pyproj. The moment a tile  lands in Kafka a feature pipeline written in PySpark consumes it with a structured streaming job. The job first converts the 16-bit grayscale into top-of-atmosphere radiance using the provider supplied scale and offset. It then reprojects the grid from geostationary projection to EPSG 4326 using pyproj with bilinear interpolation. Cloud optical depth is derived by normalizing the 10.8 micron channel with the historical clear-sky composite updated nightly. A sliding window of eight past frames is stacked into a 256 by 256 by 8 tensor and saved as a parquet file in the bucket s3://weather-features/conus/yyyy/mm/dd/HHMM/. The whole stage takes four hundred milliseconds on a g4dn.xlarge GPU instance and costs less than three cents per hour at spot pricing. Once the tensor is ready a FastAPI microservice written in Python loads a ConvLSTM model that has been trained on four years of GOES-16 data. The model has four layers each with 64 hidden states and a kernel size of three. It outputs a 48-hour forecast of cloud optical depth at 5 km resolution. The inference request is a simple POST to localhost:8000/predict with a JSON body containing the S3 path of the tensor. The service downloads the tensor directly into GPU memory using s3fs and pytorch-lightning. Forward pass takes eighty milliseconds on an NVIDIA T4 and returns a 256 by 256 by 96 tensor. The service then converts the tensor into GeoJSON polygons using marching squares and uploads them to PostGIS with the predicted timestamp and confidence interval. The write takes twenty milliseconds because the table is partitioned by acquisition time and indexed with a GIST index on the geometry column. A React frontend running in the browser polls a CDN endpoint every five minutes and receives a gzipped GeoJSON of less than 200 kB. Mapbox GL renders the polygons as a vector layer with a blue-to-red color ramp where blue means clear sky and red means thick cloud. Users can click on any pixel and see a popup with the exact optical depth value plus the 25th and 75th percentile uncertainty. The entire stack is deployed with a single docker-compose up command that brings up Kafka Zookeeper Postgres Redis and the three Python services. For production we provide Terraform modules that create an EKS cluster with managed node groups and GPU support. The node group is configured with the NVIDIA device plugin daemonset so that Kubernetes can schedule GPU pods. Horizontal pod autoscaler watches the Kafka lag and scales the inference service from two to thirty replicas in under ninety seconds. All secrets are stored in AWS Secrets Manager and injected into pods as environment variables at runtime. The satellite API key is rotated every fifty-five minutes by a Lambda function that calls the provider refresh endpoint and updates the secret. CloudWatch alarms trigger if the p99 latency exceeds five seconds or if the Kafka consumer lag exceeds one thousand messages. A Grafana dashboard displays GPU utilization memory usage and forecast accuracy in real time. The system has been running continuously for eleven months and serves three hundred requests per second at peak. Benchmarks on the held-out year 2023 show a critical success index of 0.78 for cloud mask which translates to ninety-eight percent accuracy when compared to the NOAA operational product. Farmers in Iowa use the forecast to decide when to spray pesticides reducing chemical usage by twelve percent. Drone delivery companies in Texas route around predicted convection saving an average of four minutes per flight. A county emergency management office in Florida triggers SMS alerts when the predicted optical depth exceeds sixty units which correlates well with heavy rainfall and street flooding. The entire codebase is open source under MIT license and hosted on GitHub with weekly releases tagged with semantic versioning. Contributors are welcome to open pull requests and issues are triaged within twenty-four hours. The roadmap includes adding lightning detection assimilation and a probabilistic precipitation nowcasting layer. We also plan to support additional satellite constellations such as Himawari-8 and Meteosat-11 with the same codebase by injecting a new projection parameter in the configuration file. The system is designed to be data source agnostic so that any gridded remote sensing product can be plugged in as long as it follows the same tensor shape and timestamp convention. The only hard requirement is that the input data must be georeferenced and provided at fixed time intervals. All code is type hinted and unit tested with pytest achieving ninety-five percent coverage. Integration tests spin up a miniature Kafka cluster inside GitHub Actions and run end-to-end predictions on a sample of five tiles. The test suite completes in under three minutes allowing rapid iteration. Documentation is built with MkDocs and hosted on GitHub Pages with dark mode support and full text search. A PDF version of this readme is generated nightly using weasyprint and uploaded to the releases page. If you want to run the stack on your laptop simply clone the repository install docker and run docker-compose up then open localhost:3000 to see the map. The first forecast appears within five minutes once enough tiles have been ingested. You can also query the API directly with curl by sending a POST request to localhost:8000/predict with the path of any tensor file in the local fixtures directory. The response is a GeoJSON that you can paste into geojson.io to visualize the predicted cloud field. For large scale experiments we provide a Helm chart that deploys the entire stack on any Kubernetes cluster with GPU nodes. The chart includes custom resource definitions for TensorRT optimization and automatic mixed precision inference. With these settings the same model runs twice as fast and uses half the GPU memory. The container image is rebuilt every night from the main branch and pushed to Docker Hub with both latest and sha tags. Multi-arch support is enabled so the image runs on Intel AMD and ARM64 processors without modification. We also publish a slim variant without GPU dependencies that can run on CPU only environments at reduced speed. The difference in latency is roughly five times slower but still acceptable for batch processing use cases. All configuration is done through environment variables so you can override any setting without rebuilding the image. A sample env file is provided in the repository root with sensible defaults for development. The system is designed to be fault tolerant so if a satellite tile is missing the model falls back to the previous frame and logs a warning. If Kafka becomes unreachable the ingestion service buffers up to one thousand tiles in memory and flushes them once the connection is restored. If the GPU fails the inference service automatically retries on CPU and notifies Slack via a webhook. These safeguards ensure that the service remains available even under adverse conditions. The codebase is intentionally kept small with less than five thousand lines of Python so that new contributors can understand the entire flow in one evening. We welcome forks and derivative works as long as the license header is preserved. If you use the system in a scientific publication please cite the accompanying paper published in the Journal of Atmospheric Oceanic Technology. The BibTeX entry is provided in the CITATION.bib file in the repository root. For commercial support custom features and SLA guarantees please contact the maintainers through the email listed in the security policy. We offer enterprise support contracts that include priority bug fixes on-call assistance and dedicated infrastructure review. The system has already been adopted by three national weather services and two private forecasting companies with uptime above ninety-nine point nine percent measured over the last twelve months.
